\newpageon{108}

### Developing software for the space shuttle

During 1973 and 1974 the first requirements began to be
specified for what has become one of the most interesting software
systems ever designed. It was obvious from the very beginning that
developing the Shuttle's software would be a complicated job. Even
though NASA engineers estimated the size of the flight software to be
smaller than that on Apollo, the ubiquitous functions of the Shuttle
computers meant that no one group of engineers and no one company could
do the software on its own. This increased the size of the task because
of the communication necessary between the working groups. It also
increased the complexity of a spacecraft already made complex by flight
requirements and redundancy. Besides these realities, no one could
foresee the final form that the software for this pioneering vehicle
would take, even after years of development work had elapsed, since
there continued to be both minor and major changes. NASA and its
contractors made over 2,000 requirements changes between 1975 and the
first flight in 1981**^[80](Source4.html)^**. As a result, about \$200
million was spent on software, as opposed to an initial estimate of \$20
million. Even so, NASA lessened the difficulties by making several early
decisions that were crucial for the program's success. NASA separated
the software contract from the hardware contract, closely managed the
contractors and their methods, chose a high-level language, and
maintained conceptual integrity.

NASA awarded IBM Corporation the first independent Shuttle software
contract on March 10, 1973. IBM and Rockwell International had worked
together during the period of competition for the orbiter
contract**^[81](Source4.html)^**. Rockwell bid on the entire
aerospacecraft, intending to subcontract the computer hardware and
software to IBM. But to Rockwell's dismay, NASA decided to separate the
software contract from the orbiter contract. As a result, Rockwell still
subcontracted with IBM for the computers, but IBM hand a separate
software contract monitored closely by the Spacecraft Software Division
of the Johnson Space Center. There are several reasons why this division
of labor occurred. Since software does not weigh anything in and of
itself, it is used to overcome hardware problems that would require
extra systems and components (such as a mechanical control
system)**^[82](Source4.html)^**. Thus software is in many ways the most
critical component of the Shuttle, as it ties the other components
together. Its importance to the overall program alone justified a
separate contract, since it made the contractor directly accountable to
NASA. Moreover, during the operations phase, software underwent the most
changes, the hardware being essentially fixed**^[83](Source4.html)^**.
As time went on, Rockwell's responsibilities as \newpageon{109} prime
hardware contractor were phased out, and the shuttles were turned over
to an operations group. In late 1983, Lockheed Corporation, not
Rockwell, won the competition for the operations contract. By keeping
the software contract separate, NASA could develop the code on a
continuing basis. There is a considerable difference between changing
maintenance mechanics on an existing hardware system and changing
software companies on a not yet perfect system because to date the
relationships between components in software are much harder to define
than those in hardware. Personnel experienced with a specific software
system are the best people to maintain it. Lastly, Christopher Kraft of
Johnson Space Center and George Low of NASA Headquarters, both highly
influential in the manned spacecraft program during the early 1970's,
felt that Johnson had the software management expertise to handle the
contract directly**^[84](Source4.html)^**.

One of the lessons learned from monitoring Draper Laboratory in the
Apollo era was that by having the software development at a remote site
(like Cambridge), the synergism of informally exchanged ideas is lost;
sometimes it took 3 to 4 weeks for new concepts to filter
over**^[85](Source4.html)^**. IBM had a building and several hundred
personnel near Johnson because of its Mission Control Center contracts.
When IBM won the Shuttle contract, it simply increased its local force.

The closeness of IBM to Johnson Space Center also facilitated the
ability of NASA to manage the project. The first chief of the Shuttle's
software, Richard Parten, observed that the experience of NASA managers
made a significant contribution to the success of the programming
effort**^[86](Source4.html)^**. Although IBM was a giant in the data
processing industry, a pioneer in real time systems, and capable of
putting very bright people on a project, the company had little direct
experience with avionics software. As a consequence, Rockwell had to
supply a lot of information relating to flight control. Conversely, even
though Rockwell projects used computers, software development on the
scale needed for the Shuttle was outside its experience. NASA Shuttle
managers provided the initial requirements for the software and
facilitated the exchange of information between the principal
contractors. This situation was similar to that during the 1960s when
NASA had the best rendezvous calculations people in the world and had to
contribute that expertise to IBM during the Gemini software development.
Furthermore, the lessons of Apollo inspired the NASA managers to push
IBM for quality at every point**^[87](Source4.html)^**.

The choice of a high level language for doing the majority of the coding
was important because, as Parten noted, with all the changes, "we'd
still be trying to get the thing off the ground if we'd used assembly
language"**^[88](Source4.html)^**. Programs written in high level
languages are far easier to modify. Most of the operating system
software, which is rarely changed, is in assembler, but all applications
software and some of the interfaces and redundancy management code is in
HAL/S**^[89](Source4.html)^**.

\newpageon{110} Although the decision to program in a high-level language
meant that a large amount of support software and development tools had
to be written, the high-level language nonetheless proved advantageous,
especially since it has specific statements created for real-time
programming.

#### Defining the Shuttle Software

In the end, the success of the Shuttle's software development was due to
the conceptual integrity established by using rigorously maintained
requirements documents. The requirements phase is the beginning of the
software life cycle, when the actual functions, goals, and user
interfaces of the eventual software are determined in full detail. If a
team of a thousand workers was asked to set software requirements, chaos
would result**^[90](Source4.html)^**. On the other hand, if few do the
requirements but many can alter them later, then chaos would reign
again. The strategy of using a few minds to create the software
architecture and interfaces and then ensuring that their ideas and
theirs alone are implemented, is termed "maintaining conceptual
integrity," which is well explained in Frederick C. Brooks' *The
Mythical Man Month***^[91](Source4.html)^**. As for other possible
solutions, Parten says, "the only right answer is the one you pick and
make to work"**^[92](Source4.html)^**.

Shuttle requirements documents were arranged in three Levels: A, B, and
C, the first two written by Johnson Space Center engineers. John R.
Garman prepared the Level A document, which is comprised of a
comprehensive description of the operating system, applications
programs, keyboards, displays, and other components of the software
system and its interfaces. William Sullivan wrote the guidance,
navigation and control requirements, and John Aaron, the system
management and payload specifications of Level B. They were assisted by
James Broadfoot and Robert Ernull**^[93](Source4.html)^**. Level B
requirements are different in that they are more detailed in terms of
what functions are executed when and what parameters are
needed**^[94](Source4.html)^**. The Level Bs also define what
information is to be kept in COMPOOLS, which are HAL/S structures for
maintaining common data accessed by different
tasks**^[95](Source4.html)^**. The Level C requirements were more of a
design document, forming a set with Level B requirements, since each end
item at Level C must be traceable to a Level B
requirement**^[96](Source4.html)^**. Rockwell International was
responsible for the development of the Level C require ments as,
technically, this is where the contractors take over from the customer,
NASA, in developing the software.

Early in the program, however, Draper Laboratory had significant
influence on the software and hardware systems for the Shuttle. Draper
was retained as a consultant by NASA and contributed two \newpageon{111} key
items to the software development process. The first was a document that
"taught" NASA and other contractors how to write require ments for
software, how to develop test plans, and how to use func tional flow
diagrams, among other tools[^4-5a] operating system
developed at Bell Laboratories in the 1970s, under which single function
software tools can be combined to perform a large variety of functions.

Draper's second contribution was the actual writing of some early Level
C requirements as a model**^[98](Source4.html)^**. This version of the
Level C documents contained the same components as in the later versions
delivered by Rockwell to IBM for coding. Rockwell's editions, however,
were much more detailed and complete, reflecting their practical, rather
than theoretical purpose and have been an irritation for IBM. IBM and
NASA managers suspect that Rockwell, miffed when the software contract
was taken away from them, may have delivered incredibly precise and
detailed specifications to the software contractor. These include
descriptions of flight events for each major portion of the software, a
structure chart of tasks to be done by the software during that major
segment, a functional data flowchart, and, for each module, its name,
calculations, and operations to be performed, and input and output lists
of parameters, the latter already named and accompanied by a short
definition, source, precision, and what units each are in. This is why
one NASA manager said that "you can't see the forest for the trees" in
Level C, oriented as it is to the production of individual
modules**^[99](Source4.html)^**. One IBM engineer claimed that Rockwell
went "way too far" in the Level C documents, that they told IBM too much
about how to do things rather than just what to
do**^[100](Source4.html)^**. He further claimed that the early portion
of the Shuttle development was "underengineered" and that Rockwell and
Draper included some requirements that were not passed on by NASA.
Parten, though, said that all requirements documents were subject to
regular review by joint teams from NASA and
Rockwell**^[101](Source4.html)^**.

The impression one gains from documents and interviews is that both
Rockwell and IBM fell victim to the "not invented here" \newpageon{112}
syndrome: If we didn't do it, it wasn't done right. For example,
Rockwell delivered the ascent requirements, and IBM coded them to the
letter, thereby exceeding the available memory by two and a third times
and demonstrating that the requirements for ascent were excessive.
Rockwell, in return, argued for 2 years about the nature of the
operating system, calling for a strict time-sliced system, which
allocates predefined periods of time for the execution of each task and
then suspends tasks unfinished in that time period and moves on to the
next one. The system thus cycles through all scheduled tasks in a fixed
period of time, working on each in turn. Rockwell's original proposal
was for a 40-millisecond cycle with synchronization points at the end of
each**^[102](Source4.html)^**. IBM, at NASA's urging, countered with a
priority-interrupt-driven system similar to the one on Apollo Rockwell,
experienced with time-slice systems, fought this from 1973 to 1975,
convinced it would never work**^[103](Source4.html)^**.

The requirements specifications for the Shuttle eventually contained in
their three levels what is in both the specification and design stage of
the software life cycle. In this sense, they represent a fairly complete
picture of the software at an early date. This level of detail at least
permitted NASA and its contractors to have a starting point in the
development process. IBM constantly points to the number of changes and
alterations as a continuing problem, partially ameliorated by
implementing the most mature requirements
first**^[104](Source4.html)^**. Without the attempt to provide detail at
an early date, IBM would not have had any mature requirements when the
time came to code. Even now, requirements are being changed to reflect
the actual software, so they continue to be in a process of maturation.
But early development of specifications became the means by which NASA
could enforce conceptual integrity in the shuttle software.

#### Architecture of the Primary Avionics Software System

The Primary Avionics Software System, or PASS, is the software that runs
in all the Shuttle's four primary computers. PASS is divided into two
parts: system software and applications software. The system software is
the Flight Computer Operating System (FCOS), the user interface
programming, and the system control programs, whereas the applications
software is divided into guidance, navigation and control, orbiter
systems management, payload and checkout programs. Further divisions are
explained in Box 4-3.

\newpageon{113}

<div class="inbox">Box 4-3: Structure of PASS Applications Software

The PASS guidance and navigation software is divided into major
functions, dictated by mission phases, the most obvious of which are
preflight, ascent, on-orbit, and descent. The requirements state that
these major functions be called OPS, or operational sequences. (e.g.,
OPS-1 is ascent; OPS-3, descent.) Within the OPS are major modes. In
OPS-1, the first-stage burn, second-stage burn, first orbital insertion
burn, second orbital insertion burn, and the initial on-orbit coast are
major modes; transition between major modes is automatic. Since the
total mission software exceeds the capacity of the memory, OPS
transitions are normally initiated by the crew and require the OPS to be
loaded from the MMU. This caused considerable management concern over
the preservation of data, such as the state vector, needed in more than
one OPS**^[105](Source4.html)^**. NASA's solution is to keep common data
in a major function base, which resides in memory continuously and is
not overlaid by new OPS being read into the computers.

Within each OPS, there are special functions (SPECs) and display
functions (DISPs). These are available to the crew as a supplement to
the functions being performed by the current OPS. For example, the
descent software incorporates a SPEC display showing the horizontal
situation as a supplement to the OPS display showing the vertical
situation. This SPEC is obviously not available in the on-orbit OPS. A
DISP for the on-orbit OPS may show fuel cell output levels, fuel
reserves in the orbital maneuvering system, and other such information.
SPECs usually contain items that can be selected by the crew for
execution. DISPs are just what their name means, displays and not action
items. Since SPECs and DISPs have lower priority than OPS, when a big
OPS is in memory they have to be kept on the tape and rolled in when
requested**^[106](Source4.html)^**. The actual format of the SPECs,
DISPs, OPS displays, and the software that interprets crew entries on
the keyboard is in the user interface portion of the system software.

</div>

The most critical part of the system software is the FCOS. NASA,
Rockwell, and IBM solved most of the grand conceptual problems, such as
the nature of the operating system and the redundancy management scheme,
by 1975. The first task was to convert the FCOS from the proposed
40-millisecond loop operating system to a priority-driven \[113\]
system**^[107](Source4.html)^**. Priority interrupt systems are superior
to time-slice systems because they degrade gracefully when
overloaded**^[108](Source4.html)^**. In a time-slice system, if the
tasks scheduled in the current cycle get bogged down by excessive I/O
operations, they tend to slow down the total time of execution of
processes. IBM's version of the FCOS actually has cycles, but they are
similar to the ones in the Skylab system described in the previous
chapter. The minor cycle is the high-frequency cycle; tasks within it
are scheduled every 40 milliseconds. Typical tasks in this cycle are
those related to active flight control in the atmosphere. The major
cycle is 960 milliseconds, and many monitoring and system management
tasks are scheduled at that frequency**^[109](Source4.html)^**. If a
process is still running when its time to

\newpageon{114}

![Figure 4-6. A block diagram of the Shuttle flight computer software
architecture. (From NASA, Data Processing System
Workbook)](images/p114.jpg)

restart comes up due to excessive I/O or because it was
interrupted, it cancels its next cycle and finishes
up**^[110](Source4.html)^**. If a higher priority process is called when
another process is running, then the current process is interrupted and
a program status word (PSW) containing such items as the address of the
next instruction to be executed is stored until the interruption is
satisfied. The last instruction of an interrupt is to restore the old
PSW as the current PSW so that the interrupted process can
continue**^[111](Source4.html)^**. The ability to cancel processes and
to interrupt them asynchronously provides flexibility that a strict
time-slice system does not.

A key requirement of the FCOS is to handle the real-time statements in
the HAL/S language. The most important of these are SCHEDULE, which
establishes and controls the frequency of execution of processes;
TERMINATE and CANCEL, which are the opposite of SCHEDULE; and WAIT,
which conditionally suspends execution**^[112](Source4.html)^**. The
method of implementing these statements is controlled \newpageon{115} by a
separate interface control document**^[113](Source4.html)^**. SCHEDULE
is generally programmed at the beginning of each operational sequence to
set up which tasks are to be done in that software segment and how often
they are to be done. The syntax of SCHEDULE permits the programmer to
assign a frequency and priority to each task. TERMINATE and CANCEL are
used at the end of software phases or to stop an unneeded process while
others continue. For example, after the solid rocket boosters burn out
and separate, tasks monitoring them can cease while tasks monitoring the
main engines continue to run. WAIT, although handy, is avoided by IBM
because of the possibility of the software being "hung up" while waiting
for the I/O or other condition required to continue the
process**^[114](Source4.html)^.** This is called a race condition or
"deadly embrace" and is the bane of all shared resource computer
operating systems.

The FCOS and displays occupy 35K of memory at all
times**^[115](Source4.html)^**. Add the major function base and other
resident items, and about 60K of the 106K of core remains available for
the applications programs. Of the required applications programs, ascent
and descent proved the most troublesome. Fully 75% of the software
effort went into those two programs**^[116](Source4.html)^**. After the
first attempts at preparing the ascent software resulted in a 140K load,
serious code reduction began. By 1978, IBM reduced the size of the
ascent program to 116K, but NASA Headquarters demanded it be further
knocked down to 80K**^[117](Source4.html)^**. The lowest it ever got was
98,840 words (including the system software), but its size has since
crept back up to nearly the full capacity of the memory. IBM
accomplished the reduction by moving functions that could wait until
later operational sequences**^[118](Source4.html)^**. The actual figures
for the test flight series programs are in Table
4-1**^[119](Source4.html)^**. The total size of the flight test software
was 500,000 words of code. Producing it and modifying it for later
missions required the development of a complete production facility.

\newpageon{116}

Table: Table 4-1: Sizes of Software Loads in PASS.

NAME                       |  K WORDS
---------------------------|--------:
Preflight initialization   |     72.4
Preflight checkout         |     81.4
Ascent and abort           |    105.2
On-orbit                   |     83.1
On-orbit checkout          |     80.3
On-orbit system management |     84.1
Entry                      |    101.1
Mass memory utility        |     70.1


Note: Payload and rendezvous software was added later during the
operations phase.

#### Implementing PASS

NASA planned that PASS would be a continuing development process. After
the first flight programs were produced, new functions needed to be
added and adapted to changing payload and mission requirements. For
instance, over 50% of PASS modules changed during the first 12 flights
in response to requested enhancements**^[120](Source4.html)^**. To do
this work, NASA established a Software Development Laboratory at Johnson
Space Center in 1972 to prepare for the implementation of the Shuttle
programs and to make the software tools needed for efficient coding and
maintenance. The Laboratory evolved into the Software Production
Facility (SPF) in which the software development is carried on in the
operations era. Both the facilities were equipped and managed by NASA
but used largely by contractors.

The concept of a facility dedicated to the production of onboard
software surfaced in a Rand Corporation memo in early
1970**^[121](Source4.html)^**. The memo summarized a study of software
requirements for Air Force space missions during the decade of the
1970s. One reason for a government-owned and operated software factory
was that it would be easier to establish and maintain security. Most
modules developed for \newpageon{117} the Shuttle, such as the general
flight control software and memory displays, would be unclassified.
However, Department of Defense (DoD) payloads require system management
and payload management software, plus occasional special maneuvering
modules. These were expected to be classified. Also, if the software
maintenance contract moved from the original prime contractor to some
different operations contractor, it would be considerably simpler to
accomplish the transfer if the software library and development
computers were government owned and on government property. Lastly,
having such close control over existing software and new development
would eliminate some of the problems in communication, verification, and
maintenance encountered in the three previous manned programs.

Developing the SPF turned out to be as large a task as developing the
flight software itself. During the mid-1970s, IBM had as many people
doing software for the development lab as they had working on
PASS**^[122](Source4.html)^**. The ultimate purpose of the facility is
to provide a programming team with sufficient tools to prepare a
software load for a flight. This software load is what is put on to the
MMU tape that is flown on the spacecraft. In the operations era of the
1980s, over 1,000 compiled modules are available. These are fully
tested, and often previously used, versions of tasks such as main engine
throttling, memory modification, and screen displays that rarely change
from flight to flight. New, mission-specific modules for payloads or
rendezvous maneuvers are developed and tested using the SPF's
programming tools, which themselves represent more than a million lines
of code**^[123](Source4.html)^**. The selection of existing modules and
the new modules are then combined into a flight load that is subject to
further testing. NASA achieved the goal of having such an efficient
software production system through an 8-year development process when
the SPF was still the Laboratory.

In 1972, NASA studied what sort of equipment would be required for the
facility to function properly. Large mainframe computers compatible with
the AP-101 instruction set were a must. Five IBM 360/75 computers,
released from Apollo support functions, were
available**^[124](Source4.html)^**. These were the development machines
until January of 1982**^[125](Source4.html)^**. Another requirement was
for actual flight equipment on which to test developed modules. Three
AP-101 computers with associated display electronics units connected to
the 360s with a flight equipment interface device (FEID) especially
developed for the purpose. Other needed components, such as a
6-degree-of-freedom flight simulator, were implemented in
software**^[126](Source4.html)^**. The resulting group of equipment is
capable of testing the flight software by interpreting instructions,
simulating functions, and running it in the actual flight
hardware**^[127](Source4.html)^**.

In the late 1970s, NASA realized that more powerful computers were
needed as the transition was made from development to operations. The
360s filled up, so NASA considered the Shuttle Mission \newpageon{118}
Simulator (SMS), the Shuttle Avionics Instrumentation Lab (SAIL), and
the Shuttle Data Processing Center's computers as supplementary
development sites, but this idea was rejected because they were all too
busy doing their primary functions**^[128](Source4.html)^**. In 1981,
the Facility added two new IBM 3033N computers, each with 16 million
bytes of primary memory. The SPF then consisted of those mainframes, the
three AP-101 computers and the interface devices for each, 20 magnetic
tape drives, six line printers, 66 million bytes of drum memory, 23.4
billion bytes of disk memory, and 105
terminals**^[129](Source4.html)^**. NASA accomplished rehosting the
development software to the 3033s from the 360s during the last quarter
of 1981. Even this very large computer center was not enough. Plans at
the time projected on-line primary memory to grow to 100 million
bytes**^[130](Source4.html)^**, disk storage to 160 billion
bytes**^[131](Source4.html)^**, and two more interface units, display
units, and AP-101s to handle the growing DOD
business**^[132](Source4.html)^**. Additionally, terminals connected
directly to the SPF are in Cambridge, Massachusetts, and at Goddard
Space Flight Center, Marshall Space Flight Center, Kennedy Space Center,
and Rockwell International in Downey,
California**^[133](Source4.html)^**.

Future plans for the SPF included incorporating backup system software
development, then done at Rockwell, and introducing more automation.
NASA managers who experienced both Apollo and the Shuttle realize that
the operations software preparation is not enough to keep the brightest
minds sufficiently occupied. Only a new project can do that. Therefore,
the challenge facing NASA is to automate the SPF, use more existing
modules, and free people to work on other tasks. Unfortunately, the
Shuttle software still has bugs, some of which are no fault of the
flight software developers, but rather because all the tools used in the
SPF are not yet mature. One example is the compiler for HAL/S. Just days
before the STS-7 flight, in June, 1983, an IBM employee discovered that
the latest release of the compiler had a bug in it. A quick check
revealed that over 200 flight modules had been modified and recompiled
using it. All of those had to be checked for errors before the flight
could go. Such problems will continue until the basic flight modules and
development tools are no longer constantly subject to change. In the
meantime, the accuracy of the Shuttle software is dependent on the
stringent testing program conducted by IBM and NASA before each flight.

#### Verification and Change Management of the Shuttle Software

IBM established a separate line organization for the verification of the
Shuttle software. IBM's overall Shuttle manager has two managers
reporting to him, one for design and development, and one for
verification and field operations. The verification group has just
\newpageon{119} less than half the members of the development group and uses
35% of the software budget**^[134](Source4.html)^**. There are no
managerial or personnel ties to the development group, so the test team
can adopt an "adversary relationship" with the development team. The
verifiers simply assume that the software is untested when
received**^[135](Source4.html)^**. In addition, the test team can also
attempt to prove that the requirements documents are wrong in cases
where the software becomes unworkable. This enables them to act as the
"conscience" of the entire project**^[136](Source4.html)^**.

IBM began planning for the software verification while the requirements
were being completed. By starting verification activity as the software
took shape, the test group could plan its strategy and begin to write
its own books. The verification documentation consists of test
specifications and test procedures including the actual inputs to be
used and the outputs expected, even to the detail of showing the content
of the CRT screens at various points in the
test**^[137](Source4.html)^**. The software for the first flight had to
survive 1,020 of these tests**^[138](Source4.html)^**. Future flight
loads could reuse many of the test cases, but the preparation of new
ones is a continuing activity to adjust to changes in the software and
payloads, each of which must be handled in an orderly manner.

Suggestions for changes to improve the system are unusually welcome.
Anyone, astronaut, flight trainer, IBM programmer, or NASA manager, can
submit a change request**^[139](Source4.html)^**. NASA and IBM were
processing such requests at the rate of 20 per week in
1981**^[140](Source4.html)^**. Even as late as 1983 IBM kept 30 to 40
people on requirements analysis, or the evaluation of requests for
enhancements**^[141](Source4.html)^**. NASA has a corresponding change
evaluation board. Early in the program, it was chaired by Howard W.
Tindall, the Apollo software manager, who by then was head of the Data
Systems and Analysis Directorate. This turned out to be a mistake, as he
had conflicting interests**^[142](Source4.html)^**. The change control
board moved to the Shuttle program office. Due to the careful review of
changes, it takes an average of 2 years for a new requirement to get
implemented, tested, and into the field**^[143](Source4.html)^**.
Generally, requests for extra functions that would push out current
software due to memory restrictions are turned
down**^[144](Source4.html)^**.

\newpageon{120}

<div class="inbox">Box 4-4: How IBM Verifies the Shuttle Flight Software

The Shuttle software verification process actually begins before the
test group gets the software, in the sense that the development
organization conducts internal code reviews and unit tests of individual
modules and then integration tests of groups of modules as they are
assembled into a software load. There are two levels of code inspection,
or "eyeballing" the software looking for logic errors. One level of
inspection is by the coders themselves and their peer reviewers. The
second level is done by the outside verification team. This activity
resulted in over 50% of the discrepancy reports (failures of the
software to meet the specification) filed against the software, a
percentage similar to the Apollo experience and reinforcing the value of
the idea**^[145](Source4.html)^**. When the software is assembled, it is
subject to the First Article Configuration Inspection (FACI), where it
is reviewed as a complete unit for the first time. It then passes to the
outside verification group.

Because of the nature of the software as it is delivered, the
verification team concentrates on proving that it meets the customer's
requirements and that it functions at an acceptable level of
performance. Consistent with the concept that the software is assumed
untested, the verification group can go into as much detail as time and
cost allow. Primarily, the test group concentrates on single software
loads, such as ascent, on-orbit, and so forth**^[146](Source4.html)^**.
To facilitate this, it is divided into teams that specialize in the
operating system and detail, or functional verification; teams that work
on guidance, navigation, and control; and teams that certify system
performance. These groups have access to the software in the SPF, which
thus doubles as a site for both development and testing. Using tools
available in the SPF, the verification teams can use the real flight
computers for their tests (the preferred method). The testers can freeze
the execution of software on those machines in order to check
intermediate results, alter memory, and even get a log of what commands
resulted in response to what inputs**^[147](Source4.html)^**.

After the verification group has passed the software, it is given an
official Configuration Inspection and turned over to NASA. At that point
NASA assumes configuration control, and any changes must be approved
through Agency channels. Even though NASA then has the software, IBM is
not finished with it**^[148](Source4.html)^.**

\newpageon{121} The software is usually installed in the SAIL for prelaunch,
ascent, and abort simulations, the Flight Simulation Lab (FSL) in Downey
for orbit, de-orbit, and entry simulations, and the SMS for crew
training. Although these installations are not part of the preplanned
verification process, the discrepancies noted by the users of the
software in the roughly 6 months before launch help complete the testing
in a real environment. Due to the nature of real-time computer systems,
however, the software can never be fully certified, and both IBM and
NASA are aware of this**^[149](Source4.html)^**. There are simply too
many interfaces and too many opportunities for asynchronous input and
output.

</div>

Discrepancy reports cause changes in software to make it match the
requirements. Early in the program, the software found its way into the
simulators after less verification because simulators depend on software
just to be turned on. At that time, the majority of the discrepancy
reports were from the field installations. Later, the majority turned up
in the SPF**^[150](Source4.html)^**. All discrepancy reports are
formally disposed of, either by appropriate fixes to the software, or by
waiver. Richard Parten said, "Sometimes it is better to put in an 'OPS
Note' or waiver than to fix (the software). We are dependent on smart
pilots"**^[151](Source4.html)^**. If the discrepancy is noted too close
to a flight, if it requires too much expense to fix, it can be waived if
there is no immediate danger to crew safety. Each Flight Data File
carried on board lists the most important current exceptions of which
the crew must be aware. By STS-7 in June of 1983, over 200 pages of such
exceptions and their descriptions existed**^[152](Source4.html)^**. Some
will never be fixed, but the majority were addressed during the Shuttle
launch hiatus following the 51L accident in January 1986.

So, despite the well-planned and well-manned verification effort,
software bugs exist. Part of the reason is the complexity of the
real-time system, and part is because, as one IBM manager said, "we
didn't do it up front enough," the "it" being thinking through the
program logic and verification schemes**^[153](Source4.html)^**. Aware
that effort expended at the early part of a project on quality would be
much cheaper and simpler than trying to put quality in toward the end,
IBM and NASA tried to do much more at the beginning of the Shuttle
software development than in any previous effort, but it still was not
enough to ensure perfection.

\newpageon{122}

<div class="inbox">Box 4-5: The Nature of the Backup Flight System

The Backup Flight System consists of a single computer and a software
load that contains sufficient functions to handle ascent to orbit,
selected aborts during ascent, and descent from orbit to landing site.
In the interest of avoiding a generic software failure, NASA kept its
development separate from PASS. An engineering directorate, not the
on-board software division, managed the software contract for the
backup, won by Rockwell**^[154](Source4.html)^**.

The major functional difference between PASS and the backup is that the
latter uses a time-slice operating system rather than the asynchronous
priority-driven system of PASS**^[155](Source4.html)^**. This is
consistent with Rockwell's opinion on how that system was to be
designed. Ironically, since the backup must listen in on PASS operations
so as to be ready for instant takeover, PASS had to be modified to make
it more synchronous**^[156](Source4.html)^**. Sixty engineers were still
working on the Backup Flight System software as late as
1983**^[157](Source4.html)^**.

</div>

[^4-5a]: UNIX is a trademark of AT&T.
